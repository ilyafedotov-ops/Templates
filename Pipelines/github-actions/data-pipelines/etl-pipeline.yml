name: ETL Data Pipeline

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'data/**'
      - 'pipelines/**'
      - 'spark/**'
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'dev'
        type: choice
        options:
        - dev
        - staging
        - prod
      data_date:
        description: 'Data processing date (YYYY-MM-DD)'
        required: false
        default: ''
      full_refresh:
        description: 'Perform full data refresh'
        required: false
        default: false
        type: boolean

env:
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
  DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
  DATA_FACTORY_NAME: ${{ secrets.DATA_FACTORY_NAME }}
  RESOURCE_GROUP: ${{ secrets.RESOURCE_GROUP }}

jobs:
  validate-pipeline:
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
      data-date: ${{ steps.set-env.outputs.data-date }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set environment variables
        id: set-env
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
            echo "data-date=${{ github.event.inputs.data_date }}" >> $GITHUB_OUTPUT
          else
            echo "environment=dev" >> $GITHUB_OUTPUT
            echo "data-date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT
          fi

      - name: Validate configuration
        run: |
          echo "Validating pipeline configuration for environment: ${{ steps.set-env.outputs.environment }}"
          if [[ ! -f "config/${{ steps.set-env.outputs.environment }}.yaml" ]]; then
            echo "Configuration file not found for environment: ${{ steps.set-env.outputs.environment }}"
            exit 1
          fi

  data-quality-checks:
    runs-on: ubuntu-latest
    needs: validate-pipeline
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install great-expectations pandas sqlalchemy psycopg2-binary

      - name: Run pre-processing data quality checks
        run: |
          python -c "
          import great_expectations as gx
          from great_expectations.checkpoint import SimpleCheckpoint
          
          context = gx.get_context()
          
          # Define data source validation
          validator = context.get_validator(
              batch_request={
                  'datasource_name': 'raw_data_source',
                  'data_connector_name': 'default_inferred_data_connector_name',
                  'data_asset_name': 'daily_transactions',
                  'batch_identifiers': {'date': '${{ needs.validate-pipeline.outputs.data-date }}'}
              },
              expectation_suite_name='raw_data_expectations'
          )
          
          # Run expectations
          results = validator.validate()
          
          if not results['success']:
              print('Data quality checks failed!')
              for result in results['results']:
                  if not result['success']:
                      print(f'Failed: {result[\"expectation_config\"][\"expectation_type\"]}')
              exit(1)
          else:
              print('All data quality checks passed!')
          "

  spark-etl-job:
    runs-on: ubuntu-latest
    needs: [validate-pipeline, data-quality-checks]
    strategy:
      matrix:
        job: [customer_data_etl, transaction_etl, product_analytics]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Java
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Install Spark
        run: |
          wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
          tar -xzf spark-3.4.1-bin-hadoop3.tgz
          sudo mv spark-3.4.1-bin-hadoop3 /opt/spark
          echo "/opt/spark/bin" >> $GITHUB_PATH

      - name: Submit Spark job
        env:
          SPARK_HOME: /opt/spark
        run: |
          spark-submit \
            --class org.apache.spark.examples.SparkPi \
            --master local[*] \
            --deploy-mode client \
            --driver-memory 2g \
            --executor-memory 2g \
            --executor-cores 2 \
            --conf spark.sql.adaptive.enabled=true \
            --conf spark.sql.adaptive.coalescePartitions.enabled=true \
            --conf spark.sql.adaptive.skewJoin.enabled=true \
            --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
            --conf spark.sql.execution.arrow.pyspark.enabled=true \
            spark/spark-job.py \
            --job-name ${{ matrix.job }} \
            --environment ${{ needs.validate-pipeline.outputs.environment }} \
            --data-date ${{ needs.validate-pipeline.outputs.data-date }} \
            --output-path "s3a://data-lake-${{ needs.validate-pipeline.outputs.environment }}/${{ matrix.job }}"

  azure-data-factory:
    runs-on: ubuntu-latest
    needs: [validate-pipeline, data-quality-checks]
    if: contains(fromJson('["staging", "prod"]'), needs.validate-pipeline.outputs.environment)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: |
            {
              "clientId": "${{ secrets.AZURE_CLIENT_ID }}",
              "clientSecret": "${{ secrets.AZURE_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.AZURE_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.AZURE_TENANT_ID }}"
            }

      - name: Trigger ADF Pipeline
        run: |
          az datafactory pipeline create-run \
            --factory-name ${{ env.DATA_FACTORY_NAME }} \
            --resource-group ${{ env.RESOURCE_GROUP }} \
            --name "etl-data-pipeline" \
            --parameters '{
              "environment": "${{ needs.validate-pipeline.outputs.environment }}",
              "dataDate": "${{ needs.validate-pipeline.outputs.data-date }}",
              "fullRefresh": "${{ github.event.inputs.full_refresh || false }}"
            }'

      - name: Monitor pipeline execution
        run: |
          RUN_ID=$(az datafactory pipeline create-run \
            --factory-name ${{ env.DATA_FACTORY_NAME }} \
            --resource-group ${{ env.RESOURCE_GROUP }} \
            --name "etl-data-pipeline" \
            --parameters '{"environment": "${{ needs.validate-pipeline.outputs.environment }}"}' \
            --query 'runId' -o tsv)
          
          echo "Pipeline run ID: $RUN_ID"
          
          while true; do
            STATUS=$(az datafactory pipeline-run show \
              --factory-name ${{ env.DATA_FACTORY_NAME }} \
              --resource-group ${{ env.RESOURCE_GROUP }} \
              --run-id $RUN_ID \
              --query 'status' -o tsv)
            
            echo "Pipeline status: $STATUS"
            
            if [[ "$STATUS" == "Succeeded" ]]; then
              echo "Pipeline completed successfully"
              break
            elif [[ "$STATUS" == "Failed" || "$STATUS" == "Cancelled" ]]; then
              echo "Pipeline failed with status: $STATUS"
              exit 1
            fi
            
            sleep 30
          done

  aws-glue-job:
    runs-on: ubuntu-latest
    needs: [validate-pipeline, data-quality-checks]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Start Glue Job
        run: |
          JOB_RUN_ID=$(aws glue start-job-run \
            --job-name "data-pipeline-etl" \
            --arguments '{
              "--environment":"${{ needs.validate-pipeline.outputs.environment }}",
              "--data-date":"${{ needs.validate-pipeline.outputs.data-date }}",
              "--full-refresh":"${{ github.event.inputs.full_refresh || false }}"
            }' \
            --query 'JobRunId' --output text)
          
          echo "Glue job run ID: $JOB_RUN_ID"
          
          while true; do
            STATUS=$(aws glue get-job-run \
              --job-name "data-pipeline-etl" \
              --run-id $JOB_RUN_ID \
              --query 'JobRun.JobRunState' --output text)
            
            echo "Job status: $STATUS"
            
            if [[ "$STATUS" == "SUCCEEDED" ]]; then
              echo "Glue job completed successfully"
              break
            elif [[ "$STATUS" == "FAILED" || "$STATUS" == "STOPPED" ]]; then
              echo "Glue job failed with status: $STATUS"
              aws glue get-job-run \
                --job-name "data-pipeline-etl" \
                --run-id $JOB_RUN_ID \
                --query 'JobRun.ErrorMessage'
              exit 1
            fi
            
            sleep 30
          done

  dbt-transformations:
    runs-on: ubuntu-latest
    needs: [spark-etl-job, azure-data-factory, aws-glue-job]
    if: always() && (needs.spark-etl-job.result == 'success' || needs.azure-data-factory.result == 'success' || needs.aws-glue-job.result == 'success')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake dbt-redshift dbt-bigquery

      - name: Run dbt transformations
        run: |
          cd dbt
          dbt deps
          dbt seed --target ${{ needs.validate-pipeline.outputs.environment }}
          dbt run --target ${{ needs.validate-pipeline.outputs.environment }}
          dbt test --target ${{ needs.validate-pipeline.outputs.environment }}

      - name: Generate dbt docs
        run: |
          cd dbt
          dbt docs generate --target ${{ needs.validate-pipeline.outputs.environment }}

      - name: Upload dbt artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dbt-artifacts-${{ needs.validate-pipeline.outputs.environment }}
          path: |
            dbt/target/manifest.json
            dbt/target/catalog.json
            dbt/target/run_results.json

  data-lineage:
    runs-on: ubuntu-latest
    needs: [dbt-transformations]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install data lineage tools
        run: |
          pip install datahub-cli apache-airflow-providers-datahub

      - name: Extract and push lineage metadata
        run: |
          datahub ingest -c datahub/datahub_config.yml

  post-processing-validation:
    runs-on: ubuntu-latest
    needs: [dbt-transformations]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install great-expectations pandas sqlalchemy

      - name: Run post-processing data quality checks
        run: |
          python -c "
          import great_expectations as gx
          
          context = gx.get_context()
          
          validator = context.get_validator(
              batch_request={
                  'datasource_name': 'processed_data_source',
                  'data_connector_name': 'default_inferred_data_connector_name',
                  'data_asset_name': 'processed_transactions',
                  'batch_identifiers': {'date': '${{ needs.validate-pipeline.outputs.data-date }}'}
              },
              expectation_suite_name='processed_data_expectations'
          )
          
          results = validator.validate()
          
          if not results['success']:
              print('Post-processing data quality checks failed!')
              exit(1)
          else:
              print('All post-processing data quality checks passed!')
          "

  cost-optimization:
    runs-on: ubuntu-latest
    needs: [post-processing-validation]
    if: needs.validate-pipeline.outputs.environment == 'prod'
    steps:
      - name: Cleanup temporary resources
        run: |
          echo "Cleaning up temporary compute resources"
          
          # Azure cleanup
          az vm deallocate --resource-group temp-processing-rg --name temp-processing-vm || true
          
          # AWS cleanup  
          aws ec2 terminate-instances --instance-ids $(aws ec2 describe-instances \
            --filters "Name=tag:Purpose,Values=temp-processing" \
            --query 'Reservations[].Instances[].InstanceId' --output text) || true

  notification:
    runs-on: ubuntu-latest
    needs: [post-processing-validation, cost-optimization]
    if: always()
    steps:
      - name: Notify on success
        if: needs.post-processing-validation.result == 'success'
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{"text":"✅ ETL Pipeline completed successfully for environment: ${{ needs.validate-pipeline.outputs.environment }} on date: ${{ needs.validate-pipeline.outputs.data-date }}"}' \
            ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Notify on failure
        if: failure()
        run: |
          curl -X POST -H 'Content-type: application/json' \
            --data '{"text":"❌ ETL Pipeline failed for environment: ${{ needs.validate-pipeline.outputs.environment }}. Check logs: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"}' \
            ${{ secrets.SLACK_WEBHOOK_URL }}